{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1StdNZcyu7Ss3eE9Rdxs5sG8Em4axybHj",
      "authorship_tag": "ABX9TyO6XV/m/HV7T5Btacya20Ua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rafly-Aziz/Tugas-Besar-AI-130-141/blob/main/Tubes_141_130.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VZ5FLnj5JDW",
        "outputId": "9390a089-4b11-4d5a-f3a4-0eacf446a189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract file zip dataset\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/MyDrive/UAS/Dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n"
      ],
      "metadata": {
        "id": "74irSfYm5NUC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/tmp/Dataset'\n",
        "train_dir = os.path.join(base_dir, 'Train')\n",
        "val_dir = os.path.join(base_dir, 'Validation')\n"
      ],
      "metadata": {
        "id": "-TscOvAY5Phn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NORMAL_train_path = train_dir + '/NORMAL'\n",
        "PNEUMONIA_train_path = train_dir + '/PNEUMONIA'\n",
        "NORMAL_val_path = val_dir + '/NORMAL'\n",
        "PNEUMONIA_val_path = val_dir + '/PNEUMONIA'\n",
        "\n",
        "\n",
        "NORMAL_len_train = len(os.listdir(NORMAL_train_path))\n",
        "PNEUMONIA_len_train = len(os.listdir(PNEUMONIA_train_path))\n",
        "NORMAL_len_val = len(os.listdir(NORMAL_val_path))\n",
        "PNEUMONIA_len_val = len(os.listdir(PNEUMONIA_val_path))\n",
        "\n",
        "print(\"jumlah dataset Training : \", NORMAL_len_train + PNEUMONIA_len_train)\n",
        "print(\"jumlah dataset validasi : \", NORMAL_len_val + PNEUMONIA_len_val)\n",
        "print(\"\\n\\n\")\n",
        "print(\"jumlah train kelas NORMAL : \", NORMAL_len_train)\n",
        "print(\"jumlah train kelas PNEUMONIA : \", PNEUMONIA_len_train)\n",
        "print(\"jumlah validasi kelas NORMAL : \", NORMAL_len_val)\n",
        "print(\"jumlah validasi kelas PNEUMONIA : \", PNEUMONIA_len_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2_4o7Ye5SW3",
        "outputId": "27ac767b-8bae-4867-e0e4-4090342030ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jumlah dataset Training :  148\n",
            "jumlah dataset validasi :  40\n",
            "\n",
            "\n",
            "\n",
            "jumlah train kelas NORMAL :  74\n",
            "jumlah train kelas PNEUMONIA :  74\n",
            "jumlah validasi kelas NORMAL :  20\n",
            "jumlah validasi kelas PNEUMONIA :  20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "5DqWjldq5VUR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MhWQtSy5Yo3",
        "outputId": "0cb2c313-7d2c-45a1-eb3f-6b1acc48b1be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 148 images belonging to 2 classes.\n",
            "Found 40 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Import Library yang dibutuhkan\n",
        "'''\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Dense, Flatten,Dropout, BatchNormalization\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "mfbkjYS25bHj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "definisikan dan setting callback untuk :\n",
        "1. memantau performa model\n",
        "2. melakukan save model terbaik menggunakan model checkpoint\n",
        "3. memberhentikan pelatihan ketika tidak memnuhi syarat dalam parameter earlystopping\n",
        "'''\n",
        "\n",
        "callbacks = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='auto')        \n",
        "directory_to_save_best_model_file = '/content/gdrive/MyDrive/Colab Notebooks/TugasBesar/model_drop_batch_weight_from_callback_2.h5'\n",
        "best_model = ModelCheckpoint(directory_to_save_best_model_file, monitor='val_accuracy', verbose = 1, save_best_only = True)"
      ],
      "metadata": {
        "id": "0ovy44gN5eTu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "definisikan pretrained model yang ingin digunakan\n",
        "'''\n",
        "\n",
        "VGG16_base = tf.keras.applications.VGG16(include_top=False, weights='imagenet', #include_top = false , berarti fully connected layer akan dipidah dari arsitektur\n",
        "                                                 input_tensor=None, input_shape=(224, 224,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xb9Iq405gdT",
        "outputId": "75c4b14c-a37d-40f9-9db9-0f3d233424f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "melakukan setting bahwa model pretrained tidak akan dilatih ulang\n",
        "'''\n",
        "\n",
        "VGG16_base.trainable = False"
      ],
      "metadata": {
        "id": "-vnjkDVR5jse"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "mendefinisikan layer pada bagian fully connected layer\n",
        "'''\n",
        "\n",
        "print('Adding new layers')\n",
        "output = VGG16_base.get_layer(index = -1).output  \n",
        "output = Flatten()(output)\n",
        "output = Dense(256,activation = \"relu\")(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Dropout(0.5)(output)\n",
        "output = Dense(256,activation = \"relu\")(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Dropout(0.5)(output)\n",
        "output = Dense(1, activation='sigmoid')(output) \n",
        "print('New layers Finishing Added!!!!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1tLdKPe5lmd",
        "outputId": "b53d9d76-615c-411b-8c38-27799b662ef9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding new layers\n",
            "New layers Finishing Added!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VGG16_model = Model(VGG16_base.input, output)\n",
        "\n",
        "VGG16_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E--HwtQQ5nx7",
        "outputId": "9a626bee-e3b7-47b2-b9ef-072ab7b53c19"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               6422784   \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 256)              1024      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,205,569\n",
            "Trainable params: 6,489,857\n",
            "Non-trainable params: 14,715,712\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VGG16_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', \n",
        "                        metrics =['accuracy'])"
      ],
      "metadata": {
        "id": "8Wm1v_TJ5rOZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = VGG16_model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=7,  # images = batch_size * steps\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=2,  #  images = batch_size * steps\n",
        "      callbacks = [callbacks, best_model])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaNttJS_5uZd",
        "outputId": "2a38a05a-7d63-4b78-ec32-f72ef97f4104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.7318 - accuracy: 0.6797\n",
            "Epoch 1: val_accuracy improved from -inf to 0.57500, saving model to /content/gdrive/MyDrive/Colab Notebooks/TugasBesar/model_drop_batch_weight_from_callback_2.h5\n",
            "7/7 [==============================] - 96s 14s/step - loss: 0.7318 - accuracy: 0.6797 - val_loss: 0.6123 - val_accuracy: 0.5750\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4419 - accuracy: 0.7812 \n",
            "Epoch 2: val_accuracy improved from 0.57500 to 0.90000, saving model to /content/gdrive/MyDrive/Colab Notebooks/TugasBesar/model_drop_batch_weight_from_callback_2.h5\n",
            "7/7 [==============================] - 112s 18s/step - loss: 0.4419 - accuracy: 0.7812 - val_loss: 0.5054 - val_accuracy: 0.9000\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3607 - accuracy: 0.8429 \n",
            "Epoch 3: val_accuracy did not improve from 0.90000\n",
            "7/7 [==============================] - 98s 14s/step - loss: 0.3607 - accuracy: 0.8429 - val_loss: 0.4600 - val_accuracy: 0.9000\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4772 - accuracy: 0.8125\n",
            "Epoch 4: val_accuracy did not improve from 0.90000\n",
            "7/7 [==============================] - 111s 16s/step - loss: 0.4772 - accuracy: 0.8125 - val_loss: 0.4345 - val_accuracy: 0.9000\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2977 - accuracy: 0.8828\n",
            "Epoch 5: val_accuracy did not improve from 0.90000\n",
            "7/7 [==============================] - 91s 13s/step - loss: 0.2977 - accuracy: 0.8828 - val_loss: 0.4480 - val_accuracy: 0.8250\n",
            "Epoch 6/100\n",
            "6/7 [========================>.....] - ETA: 10s - loss: 0.3328 - accuracy: 0.8333"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YlhY4XGb50O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import some of library that we need to look the confusion matrix, recall, f1_score, and accuracy score to look how much your model is well\n",
        "import numpy as np \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import precision_score, confusion_matrix, classification_report\n",
        "from sklearn import metrics\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid')"
      ],
      "metadata": {
        "id": "eYV1IQdD51Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading model to evaluate more depth\n",
        "from keras.models import load_model\n",
        "model_path =  '/content/gdrive/MyDrive/Colab Notebooks/TugasBesar/model_drop_batch_weight_from_callback_2.h5'\n",
        "model = load_model(model_path)"
      ],
      "metadata": {
        "id": "chWcdOGv53y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_metrics(y_true, y_pred):\n",
        "    accuracy=accuracy_score(y_true, y_pred)\n",
        "    precision=precision_score(y_true, y_pred,average='weighted')\n",
        "    f1Score=f1_score(y_true, y_pred, average='weighted') \n",
        "    print(\"Accuracy  : {}\".format(accuracy))\n",
        "    print(\"Precision : {}\".format(precision))\n",
        "    print(\"f1Score : {}\".format(f1Score))\n",
        "    cm=confusion_matrix(y_true, y_pred)\n",
        "    print(cm)\n",
        "    return accuracy, precision, f1Score\n",
        "\n",
        "height=224; width=224\n",
        "batch_size=20\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "TESTING_DIR = val_dir\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(TESTING_DIR,\n",
        "                                                  batch_size=batch_size,                                                             \n",
        "                                                  target_size=(height, width),\n",
        "                                                  class_mode= None,\n",
        "                                                  shuffle=False\n",
        "                                                  )\n",
        "\n",
        "predictions = model.predict_generator(generator=test_generator)\n",
        "yPredictions = predictions > 0.5\n",
        "true_classes = test_generator.classes\n",
        "class_names = test_generator.class_indices\n",
        "Cmatrix_test = confusion_matrix(true_classes, yPredictions)\n",
        "\n",
        "testAcc,testPrec, testFScore = my_metrics(true_classes, yPredictions)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "ax= plt.subplot()\n",
        "data = np.asarray(Cmatrix_test).reshape(2,2)\n",
        "sns.heatmap(data,annot=True, fmt='',ax=ax, cmap=plt.cm.Reds)\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels') \n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(class_names)   \n",
        "ax.yaxis.set_ticklabels(class_names)\n",
        "plt.title('Confusion Matrix Test',fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PUpqGGWC56Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(true_classes, yPredictions, target_names=class_names))"
      ],
      "metadata": {
        "id": "BnHPyxyi58YQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}